{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a8dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from transformers import  CLIPModel\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Data_example/training'\n",
    "gallery_dir = 'data/Data_example/test/gallery'\n",
    "query_dir = 'data/Data_example/test/query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529d1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CLIPImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor):\n",
    "        \"\"\"\n",
    "        image_dir: directory con immagini\n",
    "        processor: istanza di CLIPProcessor da Hugging Face\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, fname)\n",
    "            for fname in os.listdir(image_dir)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Usa il processor CLIP per ottenere pixel_values\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # Remove batch dim\n",
    "\n",
    "        return pixel_values, img_path  # Ritorna tensor e percorso per tracciamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994763d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd49c79",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ce016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations: resize, normalize, and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    # perform data augmentation: flip the image horizontally\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # rotate the image by 45 degrees\n",
    "    transforms.RandomRotation(45),\n",
    "    # convert the image to a tensor\n",
    "    transforms.ToTensor(),\n",
    "    # reshape the tensor to have two dimensions\n",
    "    transforms.Resize((224, 224)),  # Adjust to your image size\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained weights normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a681601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Istanzia il processor di CLIP\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Crea istanze del dataset aggiornato\n",
    "gallery_dataset = CLIPImageDataset(gallery_dir, processor=processor)\n",
    "query_dataset = CLIPImageDataset(query_dir, processor=processor)\n",
    "\n",
    "# Crea i DataLoader per caricare immagini in batch\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False)\n",
    "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefef9b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13de4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor):\n",
    "        \"\"\"\n",
    "        root_dir: directory con sottocartelle per ogni classe\n",
    "        processor: CLIPProcessor da Hugging Face\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.ImageFolder(root_dir)\n",
    "        self.processor = processor\n",
    "        self.class_to_idx = self.dataset.class_to_idx\n",
    "        self.imgs = self.dataset.imgs\n",
    "        self.class_indices = {class_name: [] for class_name in self.class_to_idx}\n",
    "\n",
    "        for idx, (img_path, class_idx) in enumerate(self.imgs):\n",
    "            class_name = list(self.class_to_idx.keys())[list(self.class_to_idx.values()).index(class_idx)]\n",
    "            self.class_indices[class_name].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def process_image(img_path):\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        anchor_img_path, anchor_label = self.imgs[idx]\n",
    "        anchor_image = process_image(anchor_img_path)\n",
    "\n",
    "        # Positive: altra immagine della stessa classe\n",
    "        positive_idx = random.choice(self.class_indices[\n",
    "            list(self.class_to_idx.keys())[anchor_label]\n",
    "        ])\n",
    "        positive_img_path, _ = self.imgs[positive_idx]\n",
    "        positive_image = process_image(positive_img_path)\n",
    "\n",
    "        # Negative: immagine da classe diversa\n",
    "        negative_class = random.choice(list(self.class_to_idx.keys()))\n",
    "        while negative_class == list(self.class_to_idx.keys())[anchor_label]:\n",
    "            negative_class = random.choice(list(self.class_to_idx.keys()))\n",
    "        negative_idx = random.choice(self.class_indices[negative_class])\n",
    "        negative_img_path, _ = self.imgs[negative_idx]\n",
    "        negative_image = process_image(negative_img_path)\n",
    "\n",
    "        return anchor_image, positive_image, negative_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f309e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0000\n",
      "Epoch 2/10, Loss: 0.0000\n",
      "Epoch 3/10, Loss: 0.0000\n",
      "Epoch 4/10, Loss: 0.0155\n",
      "Epoch 5/10, Loss: 0.0000\n",
      "Epoch 6/10, Loss: 0.0000\n",
      "Epoch 7/10, Loss: 0.0000\n",
      "Epoch 8/10, Loss: 0.0000\n",
      "Epoch 9/10, Loss: 0.0210\n",
      "Epoch 10/10, Loss: 0.0189\n"
     ]
    }
   ],
   "source": [
    "torch.seed()\n",
    "\n",
    "# Define the transformation (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),  # Adjust to your image size\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained weights normalization\n",
    "])\n",
    "\n",
    "# Init modello e processor\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model.to(device)  # solo parte visiva\n",
    "\n",
    "# Dataset e DataLoader\n",
    "train_dataset = CLIPTripletDataset(root_dir=train_dir, processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the TripletMarginLoss (you can adjust the margin parameter)\n",
    "triplet_loss = nn.TripletMarginLoss(margin=0.000001, p=2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Estrai embedding usando vision model (usa pooler_output)\n",
    "        anchor_emb = model(pixel_values=anchor).pooler_output\n",
    "        positive_emb = model(pixel_values=positive).pooler_output\n",
    "        negative_emb = model(pixel_values=negative).pooler_output\n",
    "\n",
    "        loss = triplet_loss(anchor_emb, positive_emb, negative_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3a1a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gallery_embeddings = []\n",
    "    query_embeddings = []\n",
    "    gallery_paths = []\n",
    "    query_paths = []\n",
    "\n",
    "    # Extract gallery embeddings\n",
    "    for pixel_values, paths in gallery_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values=pixel_values)  # model can be CLIPModel or CLIPVisionModel\n",
    "        emb = outputs.pooler_output  # (batch_size, hidden_dim)\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "\n",
    "    # Extract query embeddings\n",
    "    for pixel_values, paths in query_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        emb = outputs.pooler_output\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Retrieval Results:\n",
      "\n",
      "Query image:    data/Data_example/test/query\\4597118805213184.jpg\n",
      "Retrieved image: data/Data_example/test/gallery\\painting_085_000045.jpg\n",
      "--------------------------------------------------\n",
      "Query image:    data/Data_example/test/query\\n01855672_10973.jpg\n",
      "Retrieved image: data/Data_example/test/gallery\\n01855672_1037.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Stack all embedding batches into single numpy arrays\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)  # shape: (N_gallery, D)\n",
    "query_embeddings = np.vstack(query_embeddings)      # shape: (N_query, D)\n",
    "\n",
    "# Compute cosine similarity between each query and all gallery embeddings\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "\n",
    "# For each query, find the index of the most similar gallery image\n",
    "retrieved_indices = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "# Print top-1 retrieval results\n",
    "print(\"Top-1 Retrieval Results:\\n\")\n",
    "for i, idx in enumerate(retrieved_indices):\n",
    "    print(f\"Query image:    {query_paths[i]}\")\n",
    "    print(f\"Retrieved image: {gallery_paths[idx]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325fa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query image: data/Data_example/test/query\\4597118805213184.jpg\n",
      "Top-3 Retrieved gallery images:\n",
      "  1. data/Data_example/test/gallery\\painting_085_000045.jpg\n",
      "  2. data/Data_example/test/gallery\\painting_085_000118.jpg\n",
      "  3. data/Data_example/test/gallery\\n01855672_4393.jpg\n",
      "--------------------------------------------------\n",
      "Query image: data/Data_example/test/query\\n01855672_10973.jpg\n",
      "Top-3 Retrieved gallery images:\n",
      "  1. data/Data_example/test/gallery\\n01855672_1037.jpg\n",
      "  2. data/Data_example/test/gallery\\n01855672_4393.jpg\n",
      "  3. data/Data_example/test/gallery\\n01855672_4197.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_k_indices = np.argsort(similarity_matrix, axis=1)[:, -top_k:][:, ::-1]\n",
    "\n",
    "for i, indices in enumerate(top_k_indices):\n",
    "    print(f\"Query image: {query_paths[i]}\")\n",
    "    print(\"Top-3 Retrieved gallery images:\")\n",
    "    for rank, idx in enumerate(indices, start=1):\n",
    "        print(f\"  {rank}. {gallery_paths[idx]}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
