{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427aaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/I2ML_ImageClassifier/I2ML_ImageClassifier/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867cd086",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb635bed",
   "metadata": {},
   "source": [
    "Class to load the TRAIN data, considering the directory structure;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12040712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDatasetTrain(Dataset):\n",
    "    def __init__(self, root_dir, processor):\n",
    "        \"\"\"\n",
    "        root_dir: cartella principale con sottocartelle per ogni classe\n",
    "        processor: istanza di CLIPProcessor da Hugging Face\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Crea mappatura: nome_cartella → etichetta numerica\n",
    "        class_names = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "        for class_name in class_names:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(class_path, fname))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return pixel_values, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e893dba",
   "metadata": {},
   "source": [
    "Class to load TEST data, considering directories structure;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e617c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDatasetTest(Dataset):\n",
    "    def __init__(self, image_dir, processor):\n",
    "        \"\"\"\n",
    "        image_dir: directory con immagini\n",
    "        processor: istanza di CLIPProcessor da Hugging Face\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, fname)\n",
    "            for fname in os.listdir(image_dir)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Usa il processor CLIP per ottenere pixel_values\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # Remove batch dim\n",
    "\n",
    "        return pixel_values, img_path  # Ritorna tensor e percorso per tracciamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0956a",
   "metadata": {},
   "source": [
    "Batch sampler to N separate classes per batch (e.g. 8), K images for each class (e.g. 4) -> So the batch will have N × K images (e.g. 8 × 4 = 32 images per batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7fa2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, n_classes, n_samples_per_class):\n",
    "        \"\"\"\n",
    "        labels: lista di etichette (stessa lunghezza del dataset)\n",
    "        n_classes: quante classi diverse per batch\n",
    "        n_samples_per_class: quante immagini per classe per batch\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples_per_class = n_samples_per_class\n",
    "\n",
    "        # Costruisci mappa: label → lista di indici\n",
    "        self.label_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_to_indices[label].append(idx)\n",
    "\n",
    "        # Filtra classi che NON hanno abbastanza immagini\n",
    "        self.label_to_indices = {\n",
    "            label: idxs for label, idxs in self.label_to_indices.items()\n",
    "            if len(idxs) >= self.n_samples_per_class\n",
    "        }\n",
    "\n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "\n",
    "        if len(self.labels_set) < self.n_classes:\n",
    "            raise ValueError(f\"Numero di classi valide ({len(self.labels_set)}) \"\n",
    "                             f\"inferiore a n_classes richieste per batch ({self.n_classes})\")\n",
    "\n",
    "        # Numero massimo di batch stimato\n",
    "        self.num_batches = len(labels) // (n_classes * n_samples_per_class)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            batch = []\n",
    "            selected_classes = random.sample(self.labels_set, self.n_classes)\n",
    "\n",
    "            for cls in selected_classes:\n",
    "                indices = self.label_to_indices[cls]\n",
    "                sampled_indices = random.sample(indices, self.n_samples_per_class)\n",
    "                batch.extend(sampled_indices)\n",
    "\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0467a",
   "metadata": {},
   "source": [
    "SupConLoss extends the classical contrastive loss using class information. For each anchor (embedding), it considers all other examples of the same class as positive, and the rest as negative. The objective is:\n",
    "\n",
    "Approach embeddings of the same class.\n",
    "\n",
    "To distance embeddings of different classes.\n",
    "\n",
    "It works on a balanced batch like the one we built with the BalancedBatchSampler.\n",
    "\n",
    "Breakdown of the code:\n",
    "| Passaggio               | Funzione                                              |\n",
    "| ----------------------- | ----------------------------------------------------- |\n",
    "| Maschera `mask`         | Identifica quali coppie sono positive                 |\n",
    "| `features @ features.T` | Similarità tra embeddings                             |\n",
    "| `log_prob`              | Probabilità (softmax) che due embeddings siano simili |\n",
    "| `mean_log_prob_pos`     | Log-probabilità media dei veri positivi               |\n",
    "| `loss`                  | Media negativa → da minimizzare                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419bd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        features: tensor (B, D), già normalizzato (embedding L2-normalizzati)\n",
    "        labels: tensor (B,) con etichette\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)  # (B, B)\n",
    "\n",
    "        # Calcola similarità: prodotto scalare normalizzato\n",
    "        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)\n",
    "\n",
    "        # Per stabilità numerica\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # Maschera: esclude confronto con se stessi\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask), 1, torch.arange(features.shape[0]).view(-1, 1).to(device), 0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # Calcola loss\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "\n",
    "        # Media sulle ancore (solo se ci sono positivi)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1).clamp(min=1.0)\n",
    "\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df584a2",
   "metadata": {},
   "source": [
    "CLIP Encoder che:\n",
    "\n",
    "Usa la visione di CLIP,\n",
    "\n",
    "Restituisce embedding L2-normalizzati (necessari per SupConLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed7d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"openai/clip-vit-large-patch14-336\"):\n",
    "        super(CLIPEncoder, self).__init__()\n",
    "        # Carica solo la parte visuale di CLIP\n",
    "        self.vision_encoder = CLIPModel.from_pretrained(model_name).vision_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        embeddings = outputs.pooler_output  # shape (B, D)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)  # L2-normalization\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19b661",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823b4d2",
   "metadata": {},
   "source": [
    "In a nutshell: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfcd61",
   "metadata": {},
   "source": [
    "Try a supervised contrastive loss framework to adjust CLIP. Let CLIP do the embeddings, calculate the matrix similarity score between some samples of the classes in the train. How -> one class is chosen, then images (3??) for the same class become positive pair and some 4x7??? other classes to create negative pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3042e",
   "metadata": {},
   "source": [
    "Main idea: the loss helps the backprop to tune the CLIP (Visual Trasnformers) encoder to move the embeddings of the positive pairs closer in the shared multimodal embedding space, while the embeddings of the negative pairs should be moved away from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53483707",
   "metadata": {},
   "source": [
    "### CLIP loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6be089",
   "metadata": {},
   "source": [
    "More powerful version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a30e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").vision_model.to(device)  # only visual encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9195163",
   "metadata": {},
   "source": [
    "Base Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb733a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93ebc5",
   "metadata": {},
   "source": [
    "### Train loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6dec2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =  \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf44066",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CLIPImageDatasetTrain(train_dir, processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8383b8",
   "metadata": {},
   "source": [
    "### Sampling train for SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba6a924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri del batch\n",
    "n_classes_per_batch = 4\n",
    "n_samples_per_class = 2\n",
    "\n",
    "sampler = BalancedBatchSampler(\n",
    "    labels=train_dataset.labels,\n",
    "    n_classes=n_classes_per_batch,\n",
    "    n_samples_per_class=n_samples_per_class\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10ff4e",
   "metadata": {},
   "source": [
    "### Sampled train loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a409af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=sampler,   # use batch_sampler instead of batch_size\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf997817",
   "metadata": {},
   "source": [
    "### Encoder: Clip encoder (ViT - Visual Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbed5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "encoder = CLIPEncoder().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebbd36",
   "metadata": {},
   "source": [
    "### Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f738d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(encoder.parameters(), lr=3e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e06fc1",
   "metadata": {},
   "source": [
    "### Loss declaration: Supervised Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "226c433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SupConLoss(temperature=0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebae67d",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ea499",
   "metadata": {},
   "source": [
    "Clean and simple training loop using your:\n",
    "\n",
    "- train_loader (with BalancedBatchSampler)\n",
    "\n",
    "- CLIPEncoder (with normalized embeddings)\n",
    "\n",
    "- SupConLoss\n",
    "\n",
    "- Adam optimizer\n",
    "\n",
    "We’ll use a small number of epochs (e.g., 5) just to validate that everything works smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e414e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  25%|██▍       | 153/624 [02:28<07:33,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 1  # Start small\n",
    "encoder.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for pixel_values, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = encoder(pixel_values)  # Already normalized\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(embeddings, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Logging\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecda96",
   "metadata": {},
   "source": [
    "Saving model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fine_tuned_clip_encoder.pth\"\n",
    "torch.save(encoder.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f699ab5",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80973dd2",
   "metadata": {},
   "source": [
    "Test set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_dir = 'test-2/gallery'\n",
    "query_dir = 'test-2/query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109766ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea istanze del dataset aggiornato\n",
    "gallery_dataset = CLIPImageDatasetTest(gallery_dir, processor=processor)\n",
    "query_dataset = CLIPImageDatasetTest(query_dir, processor=processor)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=8, shuffle=False)  # era 32\n",
    "query_loader = DataLoader(query_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e2ace",
   "metadata": {},
   "source": [
    "Forward evalutation for Pre-trained models without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gallery_embeddings = []\n",
    "    query_embeddings = []\n",
    "    gallery_paths = []\n",
    "    query_paths = []\n",
    "\n",
    "    print(\"Extracting gallery embeddings...\")\n",
    "    for i, (pixel_values, paths) in enumerate(tqdm(gallery_loader)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        pixel_values = pixel_values.to(device)\n",
    "\n",
    "        # Forward pass con il visual encoder CLIP (restituisce dict con 'pooler_output')\n",
    "        outputs = model(pixel_values)\n",
    "        emb = outputs.pooler_output  # shape: (batch_size, hidden_dim)\n",
    "\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        del pixel_values, emb, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    print(\"Extracting query embeddings...\")\n",
    "    for i, (pixel_values, paths) in enumerate(tqdm(query_loader)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        pixel_values = pixel_values.to(device)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        emb = outputs.pooler_output\n",
    "\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        del pixel_values, emb, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90494ae3",
   "metadata": {},
   "source": [
    "Forward evalutation for fine-tuned encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gallery_embeddings = []\n",
    "    query_embeddings = []\n",
    "    gallery_paths = []\n",
    "    query_paths = []\n",
    "\n",
    "    # Extract gallery embeddings con progress bar\n",
    "    print(\"Extracting gallery embeddings...\")\n",
    "    for i, (pixel_values, paths) in enumerate(tqdm(gallery_loader)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        pixel_values = pixel_values.to(device)\n",
    "        # Usa l'encoder fine-tuned invece del model completo\n",
    "        emb = encoder(pixel_values)  # L'encoder restituisce già embeddings normalizzati\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "        \n",
    "        # Stampa timing ogni 10 batch\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Libera memoria GPU\n",
    "        del pixel_values, emb\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Extract query embeddings\n",
    "    print(\"Extracting query embeddings...\")\n",
    "    for i, (pixel_values, paths) in enumerate(tqdm(query_loader)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        pixel_values = pixel_values.to(device)\n",
    "        # Usa l'encoder fine-tuned invece del model completo\n",
    "        emb = encoder(pixel_values)  # L'encoder restituisce già embeddings normalizzati\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "            \n",
    "        # Libera memoria GPU\n",
    "        del pixel_values, emb\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927fcdb",
   "metadata": {},
   "source": [
    "Evalutation && results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c498037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all embedding batches into single numpy arrays\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)  # shape: (N_gallery, D)\n",
    "query_embeddings = np.vstack(query_embeddings)      # shape: (N_query, D)\n",
    "\n",
    "# Compute cosine similarity between each query and all gallery embeddings\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "\n",
    "# For each query, find the index of the most similar gallery image\n",
    "retrieved_indices = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "top_k = 10\n",
    "top_k_indices = np.argsort(similarity_matrix, axis=1)[:, -top_k:][:, ::-1]\n",
    "\n",
    "# Build results dictionary in the required format\n",
    "results = {}\n",
    "\n",
    "for i, indices in enumerate(top_k_indices):\n",
    "    # Extract just the filename from the full path\n",
    "    query_filename = os.path.basename(query_paths[i])\n",
    "    \n",
    "    # Get the top-k gallery filenames\n",
    "    retrieved_filenames = [os.path.basename(gallery_paths[idx]) for idx in indices]\n",
    "    \n",
    "    results[query_filename] = retrieved_filenames\n",
    "\n",
    "# Save results to JSON file\n",
    "output_file = \"retrieval_results.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Optional: Print a few examples to verify format\n",
    "print(\"\\nFirst 3 results:\")\n",
    "for i, (query, retrieved) in enumerate(results.items()):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-3 Retrieved: {retrieved[:3]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee47b95",
   "metadata": {},
   "source": [
    "# Multiple Fine-Tuning && Forward pass + results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results and weights directories if they don't exist\n",
    "results_dir = \"results\"\n",
    "weights_dir = \"weights\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "num_epochs = 10\n",
    "encoder.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Training phase\n",
    "    print(\"Training...\")\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for pixel_values, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = encoder(pixel_values)  # Already normalized\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(embeddings, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Logging\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation phase\n",
    "    print(f\"\\nEvaluating after epoch {epoch+1}...\")\n",
    "    encoder.eval()  # Set to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gallery_embeddings = []\n",
    "        query_embeddings = []\n",
    "        gallery_paths = []\n",
    "        query_paths = []\n",
    "\n",
    "        # Extract gallery embeddings con progress bar\n",
    "        print(\"Extracting gallery embeddings...\")\n",
    "        for i, (pixel_values, paths) in enumerate(tqdm(gallery_loader, desc=\"Gallery\")):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            pixel_values = pixel_values.to(device)\n",
    "            # Usa l'encoder fine-tuned invece del model completo\n",
    "            emb = encoder(pixel_values)  # L'encoder restituisce già embeddings normalizzati\n",
    "            gallery_embeddings.append(emb.cpu().numpy())\n",
    "            gallery_paths.extend(paths)\n",
    "            \n",
    "            # Stampa timing ogni 10 batch\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "            \n",
    "            # Libera memoria GPU\n",
    "            del pixel_values, emb\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        # Extract query embeddings\n",
    "        print(\"Extracting query embeddings...\")\n",
    "        for i, (pixel_values, paths) in enumerate(tqdm(query_loader, desc=\"Query\")):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            pixel_values = pixel_values.to(device)\n",
    "            # Usa l'encoder fine-tuned invece del model completo\n",
    "            emb = encoder(pixel_values)  # L'encoder restituisce già embeddings normalizzati\n",
    "            query_embeddings.append(emb.cpu().numpy())\n",
    "            query_paths.extend(paths)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}: {time.time() - start_time:.2f}s\")\n",
    "                \n",
    "            # Libera memoria GPU\n",
    "            del pixel_values, emb\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        # Stack all embedding batches into single numpy arrays\n",
    "        gallery_embeddings = np.vstack(gallery_embeddings)  # shape: (N_gallery, D)\n",
    "        query_embeddings = np.vstack(query_embeddings)      # shape: (N_query, D)\n",
    "\n",
    "        # Compute cosine similarity between each query and all gallery embeddings\n",
    "        print(\"Computing similarities...\")\n",
    "        similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "\n",
    "        # For each query, find the top-k most similar gallery images\n",
    "        top_k = 10\n",
    "        top_k_indices = np.argsort(similarity_matrix, axis=1)[:, -top_k:][:, ::-1]\n",
    "\n",
    "        # Build results dictionary in the required format\n",
    "        results = {}\n",
    "\n",
    "        for i, indices in enumerate(top_k_indices):\n",
    "            # Extract just the filename from the full path\n",
    "            query_filename = os.path.basename(query_paths[i])\n",
    "            \n",
    "            # Get the top-k gallery filenames\n",
    "            retrieved_filenames = [os.path.basename(gallery_paths[idx]) for idx in indices]\n",
    "            \n",
    "            results[query_filename] = retrieved_filenames\n",
    "\n",
    "        # Save results to JSON file with epoch number\n",
    "        output_file = os.path.join(results_dir, f\"retrieval_results_epoch_{epoch+1:02d}.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        \n",
    "        # Save model weights after evaluation\n",
    "        weights_file = os.path.join(weights_dir, f\"encoder_epoch_{epoch+1:02d}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, weights_file)\n",
    "        print(f\"Model weights saved to {weights_file}\")\n",
    "\n",
    "        # Optional: Print a few examples to verify format\n",
    "        print(f\"\\nFirst 3 results for epoch {epoch+1}:\")\n",
    "        for i, (query, retrieved) in enumerate(results.items()):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Top-3 Retrieved: {retrieved[:3]}\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    # Set back to training mode for next epoch\n",
    "    encoder.train()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} completed!\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(f\"All results saved in '{results_dir}' directory\")\n",
    "print(f\"All model weights saved in '{weights_dir}' directory\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
