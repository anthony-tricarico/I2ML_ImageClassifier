{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determininig Model Performance on the Oxford Pet Dataset\n",
    "This notebook has been created with the intent to showcase our work in terms of determining out-of-the-box model performances on a commonly used dataset. We start by importing the necessary libraries and then we move on to code utility functions for the task and data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chihuahua', 'samoyed', 'yorkshire_terrier', 'leonberger', 'basset_hound', 'pomeranian', 'Egyptian_Mau', 'Maine_Coon', 'english_setter', 'Ragdoll', 'Abyssinian', 'newfoundland', 'boxer', 'Russian_Blue', 'Persian', 'american_bulldog', 'Siamese', 'pug', 'staffordshire_bull_terrier', 'saint_bernard', 'great_pyrenees', 'beagle', 'Birman', 'scottish_terrier', 'keeshond', 'wheaten_terrier', 'Sphynx', 'german_shorthaired', 'Bombay', 'havanese', 'miniature_pinscher', 'shiba_inu', 'english_cocker_spaniel', 'British_Shorthair', 'Bengal', 'american_pit_bull_terrier', 'japanese_chin'}\n"
     ]
    }
   ],
   "source": [
    "# Extract categories from the file directory\n",
    "categories = set()\n",
    "directory = '../oxford_pets/images'\n",
    "file_list = list(os.walk(directory))[0][2]\n",
    "\n",
    "for filename in file_list:\n",
    "    index = filename.rfind('_')\n",
    "    categories.add(filename[:index])\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = set()\n",
    "directory = '../oxford_pets/images'\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith('.jpg')]\n",
    "\n",
    "# Extract categories\n",
    "for filename in file_list:\n",
    "    index = filename.rfind('_')\n",
    "    categories.add(filename[:index])\n",
    "\n",
    "# Create query and gallery directories if they don't exist\n",
    "gallery_dir = os.path.join(directory, 'gallery')\n",
    "query_dir = os.path.join(directory, 'query')\n",
    "os.makedirs(gallery_dir, exist_ok=True)\n",
    "os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "for cat in categories:\n",
    "    filtered = [f for f in file_list if f.startswith(cat + '_')]\n",
    "    if not filtered:\n",
    "        continue\n",
    "    random.seed(1)\n",
    "    sampled_query = random.choice(filtered)\n",
    "    filtered.remove(sampled_query)\n",
    "\n",
    "    # Move sampled query image\n",
    "    src_query = os.path.join(directory, sampled_query)\n",
    "    dst_query = os.path.join(query_dir, sampled_query)\n",
    "    if os.path.exists(src_query):\n",
    "        os.rename(src_query, dst_query)\n",
    "    else:\n",
    "        print(f\"File not found: {src_query}\")\n",
    "\n",
    "    # Move gallery images\n",
    "    cat_gallery_dir = os.path.join(gallery_dir, cat)\n",
    "    os.makedirs(cat_gallery_dir, exist_ok=True)\n",
    "    for file in filtered:\n",
    "        src_gallery = os.path.join(directory, file)\n",
    "        dst_gallery = os.path.join(cat_gallery_dir, file)\n",
    "        if os.path.exists(src_gallery):\n",
    "            os.rename(src_gallery, dst_gallery)\n",
    "        else:\n",
    "            print(f\"File not found: {src_gallery}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "The first model we experimented with was ResNet due to its less convoluted structure compared to other architectures used afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Paths to gallery and query directories\n",
    "gallery_dir = '../oxford_pets/images/gallery'\n",
    "query_dir = '../oxford_pets/images/query'\n",
    "\n",
    "# 2. Dataset and DataLoader\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_paths = []\n",
    "        # Recursively collect all image files\n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root, fname))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "gallery_dataset = ImageOnlyDataset(gallery_dir, transform=transform)\n",
    "query_dataset = ImageOnlyDataset(query_dir, transform=transform)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False)\n",
    "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Load Model (ResNet18, as in your notebook)\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "# Remove the final classification layer to get embeddings\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "# Move to device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# 4. Extract Embeddings\n",
    "gallery_embeddings = []\n",
    "query_embeddings = []\n",
    "gallery_paths = []\n",
    "query_paths = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, paths in gallery_loader:\n",
    "        images = images.to(device)\n",
    "        emb = model(images)\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "    for images, paths in query_loader:\n",
    "        images = images.to(device)\n",
    "        emb = model(images)\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)\n",
    "\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)\n",
    "query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "# 5. Compute Cosine Similarity and Top-k Retrievals\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "top_k = 10 \n",
    "topk_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
    "\n",
    "def get_breed(filename):\n",
    "    base = os.path.basename(filename)\n",
    "    return base[:base.rfind('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 1.0000\n",
      "Accuracy@1: 0.9189\n",
      "Precision@10: 0.8568\n"
     ]
    }
   ],
   "source": [
    "# 6. Compute Metrics\n",
    "recall_correct = 0\n",
    "accuracy_correct = 0\n",
    "precision_sum = 0\n",
    "\n",
    "for i, indices in enumerate(topk_indices):\n",
    "    query_breed = get_breed(query_paths[i])\n",
    "    retrieved_breeds = [get_breed(gallery_paths[idx]) for idx in indices]\n",
    "    # Recall@k: at least one correct in top-k\n",
    "    if query_breed in retrieved_breeds:\n",
    "        recall_correct += 1\n",
    "    # Accuracy@1: top-1 is correct\n",
    "    if query_breed == retrieved_breeds[0]:\n",
    "        accuracy_correct += 1\n",
    "    # Precision@k: fraction of correct in top-k\n",
    "    precision_sum += retrieved_breeds.count(query_breed) / top_k\n",
    "\n",
    "recall_at_k = recall_correct / len(query_paths)\n",
    "accuracy_at_1 = accuracy_correct / len(query_paths)\n",
    "precision_at_k = precision_sum / len(query_paths)\n",
    "\n",
    "print(f\"Recall@{top_k}: {recall_at_k:.4f}\")\n",
    "print(f\"Accuracy@1: {accuracy_at_1:.4f}\")\n",
    "print(f\"Precision@{top_k}: {precision_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "In our tests, CLIP proved to be the best model due to its solid performance across all metrics considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(336, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(336),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4815, 0.4578, 0.4082], [0.2686, 0.2613, 0.2758])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after loading processor\n",
      "Number of gallery images: 7349\n",
      "after embeddings\n",
      "after similarity\n",
      "CLIP Recall@10: 1.0000\n",
      "CLIP Accuracy@1: 0.9730\n",
      "CLIP Precision@10: 0.9027\n"
     ]
    }
   ],
   "source": [
    "# 1. Paths to gallery and query directories\n",
    "gallery_dir = \"/home/disi/oxford_pets/images/gallery\"\n",
    "query_dir = '/home/disi/oxford_pets/images/query'\n",
    "\n",
    "# 2. Dataset and DataLoader for CLIP\n",
    "class CLIPImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor):\n",
    "        \"\"\"\n",
    "        image_dir: directory con immagini\n",
    "        processor: istanza di CLIPProcessor da Hugging Face\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = []\n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root, fname))\n",
    "        \n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # CLIPImageProcessor returns a tensor directly\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return pixel_values, img_path\n",
    "\n",
    "# 3. Load CLIP model and processor\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").vision_model.to(device)  # Only vision part\n",
    "\n",
    "print('after loading processor')\n",
    "# 4. DataLoaders\n",
    "def collate_fn(batch):\n",
    "    images, paths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, list(paths)\n",
    "\n",
    "gallery_dataset = CLIPImageDataset(gallery_dir, processor=processor)\n",
    "query_dataset = CLIPImageDataset(query_dir, processor=processor)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "query_loader = DataLoader(query_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of gallery images: {len(gallery_dataset)}\")\n",
    "\n",
    "# 5. Extract Embeddings\n",
    "with torch.no_grad():\n",
    "    gallery_embeddings = []\n",
    "    query_embeddings = []\n",
    "    gallery_paths = []\n",
    "    query_paths = []\n",
    "\n",
    "    for pixel_values, paths in gallery_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        emb = outputs.pooler_output\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "\n",
    "    for pixel_values, paths in query_loader:\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        emb = outputs.pooler_output\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)\n",
    "\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)\n",
    "query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "print('after embeddings')\n",
    "# 6. Compute Cosine Similarity and Top-k Retrievals\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "top_k = 10  # Change as needed\n",
    "topk_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
    "\n",
    "def get_breed(filename):\n",
    "    base = os.path.basename(filename)\n",
    "    return base[:base.rfind('_')]\n",
    "\n",
    "print('after similarity')\n",
    "# 7. Compute Metrics\n",
    "recall_correct = 0\n",
    "accuracy_correct = 0\n",
    "precision_sum = 0\n",
    "\n",
    "for i, indices in enumerate(topk_indices):\n",
    "    query_breed = get_breed(query_paths[i])\n",
    "    retrieved_breeds = [get_breed(gallery_paths[idx]) for idx in indices]\n",
    "    # Recall@k: at least one correct in top-k\n",
    "    if query_breed in retrieved_breeds:\n",
    "        recall_correct += 1\n",
    "    # Accuracy@1: top-1 is correct\n",
    "    if query_breed == retrieved_breeds[0]:\n",
    "        accuracy_correct += 1\n",
    "    # Precision@k: fraction of correct in top-k\n",
    "    precision_sum += retrieved_breeds.count(query_breed) / top_k\n",
    "\n",
    "recall_at_k = recall_correct / len(query_paths)\n",
    "accuracy_at_1 = accuracy_correct / len(query_paths)\n",
    "precision_at_k = precision_sum / len(query_paths)\n",
    "\n",
    "print(f\"CLIP Recall@{top_k}: {recall_at_k:.4f}\")\n",
    "print(f\"CLIP Accuracy@1: {accuracy_at_1:.4f}\")\n",
    "print(f\"CLIP Precision@{top_k}: {precision_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet\n",
    "We also considered EfficientNet to make sure to take into account most of the models suggested by the literature available on the topic. However, the performances of such model were mixed similar to what observed with ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/disi/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 208MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B0 Recall@10: 0.9730\n",
      "EfficientNet-B0 Accuracy@1: 0.8919\n",
      "EfficientNet-B0 Precision@10: 0.8784\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Paths to gallery and query directories -----\n",
    "gallery_dir = \"/home/disi/oxford_pets/images/gallery\"\n",
    "query_dir = '/home/disi/oxford_pets/images/query'\n",
    "\n",
    "\n",
    "# ----- 2. Dataset -----\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_paths = []\n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root, fname))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ----- 3. Image preprocessing -----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# ----- 4. Load data -----\n",
    "gallery_dataset = ImageOnlyDataset(gallery_dir, transform=transform)\n",
    "query_dataset = ImageOnlyDataset(query_dir, transform=transform)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False)\n",
    "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ----- 5. Load EfficientNetB0 and remove classifier -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model.classifier = nn.Identity()\n",
    "model.eval().to(device)\n",
    "\n",
    "# ----- 6. Extract embeddings -----\n",
    "def extract_embeddings(loader):\n",
    "    embeddings = []\n",
    "    paths = []\n",
    "    with torch.no_grad():\n",
    "        for images, batch_paths in loader:\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            embeddings.append(features.cpu().numpy())\n",
    "            paths.extend(batch_paths)\n",
    "    return np.vstack(embeddings), paths\n",
    "\n",
    "gallery_embeddings, gallery_paths = extract_embeddings(gallery_loader)\n",
    "query_embeddings, query_paths = extract_embeddings(query_loader)\n",
    "\n",
    "# ----- 7. Similarity and Ranking -----\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "top_k = 10\n",
    "topk_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
    "\n",
    "# ----- 8. Evaluation -----\n",
    "def get_label(filepath):\n",
    "    base = os.path.basename(filepath)\n",
    "    return base[:base.rfind('_')]  # assumes filenames like 'label_xxx.jpg'\n",
    "\n",
    "recall_correct = 0\n",
    "accuracy_correct = 0\n",
    "precision_sum = 0\n",
    "\n",
    "for i, indices in enumerate(topk_indices):\n",
    "    query_label = get_label(query_paths[i])\n",
    "    retrieved_labels = [get_label(gallery_paths[idx]) for idx in indices]\n",
    "\n",
    "    if query_label in retrieved_labels:\n",
    "        recall_correct += 1\n",
    "    if query_label == retrieved_labels[0]:\n",
    "        accuracy_correct += 1\n",
    "    precision_sum += retrieved_labels.count(query_label) / top_k\n",
    "\n",
    "recall_at_k = recall_correct / len(query_paths)\n",
    "accuracy_at_1 = accuracy_correct / len(query_paths)\n",
    "precision_at_k = precision_sum / len(query_paths)\n",
    "\n",
    "# ----- 9. Print results -----\n",
    "print(f\"EfficientNet-B0 Recall@{top_k}: {recall_at_k:.4f}\")\n",
    "print(f\"EfficientNet-B0 Accuracy@1: {accuracy_at_1:.4f}\")\n",
    "print(f\"EfficientNet-B0 Precision@{top_k}: {precision_at_k:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
