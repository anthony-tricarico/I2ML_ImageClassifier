{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50aa8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from utils.training import prepare_model_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6050b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../../data/training'\n",
    "gallery_dir = '../../data/test/gallery'\n",
    "query_dir = '../../data/test/query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2489371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOnlyDataset(Dataset):\n",
    "    # class constructor\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        # get image directory\n",
    "        self.image_dir = image_dir\n",
    "        # get filenames by joining the image_dir path with fname where fname is every \n",
    "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir)\n",
    "                            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # specify the transformations to be applied\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # specify this to then be able to use the len() function on objects of this class\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # specify how to get the path of the image by extacting image from the above-specified list\n",
    "        img_path = self.image_paths[idx]\n",
    "        # open image and convert it to RGB mode (not grayscale, not anything else)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # if transformations have been specified at the step above (i.e., not None)\n",
    "        if self.transform:\n",
    "            # apply transformations to the image\n",
    "            image = self.transform(image)\n",
    "        # at the end, return both the image as a PIL instance and an img_path to find it later in the folders\n",
    "        return image, img_path  # Return path so you can match later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "287e01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet-18 model\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fc81175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_training(model, finetune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be0d0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the mps to use GPU\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028f1fd",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d15a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations: resize, normalize, and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    # perform data augmentation: flip the image horizontally\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # rotate the image by 45 degrees\n",
    "    transforms.RandomRotation(45),\n",
    "    # convert the image to a tensor\n",
    "    transforms.ToTensor(),\n",
    "    # reshape the tensor to have two dimensions\n",
    "    transforms.Resize((224, 224)),  # Adjust to your image size\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained weights normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08d34415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using ImageFolder\n",
    "# train_dataset = datasets.ImageFolder(root = train_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce0dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create an instance of the ImageOnlyDataset by specifying the transformations\n",
    "gallery_dataset = ImageOnlyDataset(gallery_dir, transform=transform)\n",
    "query_dataset = ImageOnlyDataset(query_dir, transform=transform)\n",
    "\n",
    "# creates a data loader to load images in batches\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False)\n",
    "query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a80424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DataLoader for batching\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# # val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db6d90",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cad76c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f034e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ResNetEmbedding(nn.Module):\n",
    "#     # class constructor taking the resnet model and the dimension of the embeddings as input\n",
    "#     def __init__(self, resnet_model, embedding_dim):\n",
    "#         super(ResNetEmbedding, self).__init__()\n",
    "#         self.resnet = resnet_model\n",
    "\n",
    "#         # Get the number of input features to the last fully connected layer\n",
    "#         in_features = self.resnet.fc.in_features\n",
    "\n",
    "#         # Replace the final fully connected layer with an identity layer\n",
    "#         self.resnet.fc = nn.Identity()\n",
    "\n",
    "#         # Define a new fully connected layer for embedding\n",
    "#         self.fc = nn.Linear(in_features, embedding_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.resnet(x)  # Forward pass through ResNet backbone (up to before the classification layer)\n",
    "#         x = self.fc(x)      # Pass through the embedding layer\n",
    "#         return x\n",
    "\n",
    "# # Define the embedding dimension (e.g., 128)\n",
    "# embedding_dim = 128\n",
    "\n",
    "# # Load ResNet18 with pretrained weights\n",
    "# model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# # Create the custom model\n",
    "# model = ResNetEmbedding(model, embedding_dim)\n",
    "\n",
    "# # Move to device (GPU or CPU)\n",
    "# device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e3b6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the class folders.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = self.dataset.class_to_idx\n",
    "        self.imgs = self.dataset.imgs  # List of (image_path, class_index)\n",
    "        self.class_indices = {class_name: [] for class_name in self.class_to_idx.keys()}\n",
    "\n",
    "        for idx, (img_path, class_idx) in enumerate(self.imgs):\n",
    "            class_name = list(self.class_to_idx.keys())[list(self.class_to_idx.values()).index(class_idx)]\n",
    "            self.class_indices[class_name].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img_path, anchor_label = self.imgs[idx]\n",
    "        anchor_image = Image.open(anchor_img_path)\n",
    "\n",
    "        # Apply the transform (e.g., resizing, normalization)\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "\n",
    "        # Positive: A random image from the same class\n",
    "        positive_idx = random.choice(self.class_indices[list(self.class_to_idx.keys())[anchor_label]])\n",
    "        positive_img_path, positive_label = self.imgs[positive_idx]\n",
    "        positive_image = Image.open(positive_img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            positive_image = self.transform(positive_image)\n",
    "\n",
    "        # Negative: A random image from a different class\n",
    "        negative_class = random.choice(list(self.class_to_idx.keys()))\n",
    "        while negative_class == list(self.class_to_idx.keys())[anchor_label]:  # Ensure it's not the same class\n",
    "            negative_class = random.choice(list(self.class_to_idx.keys()))\n",
    "\n",
    "        negative_idx = random.choice(self.class_indices[negative_class])\n",
    "        negative_img_path, negative_label = self.imgs[negative_idx]\n",
    "        negative_image = Image.open(negative_img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            negative_image = self.transform(negative_image)\n",
    "\n",
    "        # Return the triplet\n",
    "        return anchor_image, positive_image, negative_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3a32ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),  # Adjust to your image size\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained weights normalization\n",
    "])\n",
    "\n",
    "# Create the triplet dataset and DataLoader\n",
    "train_dataset = TripletDataset(root_dir=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Now you can use this `train_loader` in your training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49867795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.498185396194458\n",
      "Epoch 2/10, Loss: 1.0565192699432373\n",
      "Epoch 3/10, Loss: 0.0\n",
      "Epoch 4/10, Loss: 0.20272397994995117\n",
      "Epoch 5/10, Loss: 5.170644521713257\n",
      "Epoch 6/10, Loss: 1.0551159381866455\n",
      "Epoch 7/10, Loss: 2.9503393173217773\n",
      "Epoch 8/10, Loss: 0.535742998123169\n",
      "Epoch 9/10, Loss: 0.7651948928833008\n",
      "Epoch 10/10, Loss: 1.1809091567993164\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.seed()\n",
    "\n",
    "# Define the TripletMarginLoss (you can adjust the margin parameter)\n",
    "triplet_loss = nn.TripletMarginLoss(margin=0.000001, p=2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        # Move the data to the GPU (if available)\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "\n",
    "        # Compute the triplet loss\n",
    "        loss = triplet_loss(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15c2d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gallery_embeddings = []\n",
    "    query_embeddings = []\n",
    "    gallery_paths = []\n",
    "    query_paths = []\n",
    "\n",
    "    # Extract gallery embeddings\n",
    "    for images, paths in gallery_loader:\n",
    "        images = images.to(device)\n",
    "        emb = model(images)\n",
    "        gallery_embeddings.append(emb.cpu().numpy())\n",
    "        gallery_paths.extend(paths)\n",
    "\n",
    "    # Extract query embeddings\n",
    "    for images, paths in query_loader:\n",
    "        images = images.to(device)\n",
    "        emb = model(images)\n",
    "        query_embeddings.append(emb.cpu().numpy())\n",
    "        query_paths.extend(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "457ab238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query image: ../../data/test/query/4597118805213184.jpg\n",
      "Retrieved gallery image: ../../data/test/gallery/painting_085_000045.jpg\n",
      "\n",
      "Query image: ../../data/test/query/n01855672_10973.jpg\n",
      "Retrieved gallery image: ../../data/test/gallery/n01855672_1037.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)\n",
    "query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_matrix = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "\n",
    "# For each query image, get the most similar gallery image\n",
    "retrieved_indices = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "# Print results\n",
    "for i, idx in enumerate(retrieved_indices):\n",
    "    print(f\"Query image: {query_paths[i]}\")\n",
    "    print(f\"Retrieved gallery image: {gallery_paths[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b78dc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a85bc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query image: ../../data/test/query/4597118805213184.jpg\n",
      "Top 5 retrieved gallery images:\n",
      "  Rank 1: ../../data/test/gallery/painting_085_000045.jpg\n",
      "  Rank 2: ../../data/test/gallery/painting_085_000118.jpg\n",
      "  Rank 3: ../../data/test/gallery/painting_085_000084.jpg\n",
      "  Rank 4: ../../data/test/gallery/n01855672_4393.jpg\n",
      "  Rank 5: ../../data/test/gallery/n01855672_1037.jpg\n",
      "\n",
      "Query image: ../../data/test/query/n01855672_10973.jpg\n",
      "Top 5 retrieved gallery images:\n",
      "  Rank 1: ../../data/test/gallery/n01855672_1037.jpg\n",
      "  Rank 2: ../../data/test/gallery/n01855672_4197.jpg\n",
      "  Rank 3: ../../data/test/gallery/n01855672_4393.jpg\n",
      "  Rank 4: ../../data/test/gallery/painting_085_000045.jpg\n",
      "  Rank 5: ../../data/test/gallery/painting_085_000084.jpg\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "gallery_embeddings = np.vstack(gallery_embeddings)\n",
    "query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "top_k = 5  # You can change this to any value (e.g., 1, 3, 10)\n",
    "\n",
    "# Compute top-k most similar gallery indices for each query\n",
    "topk_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
    "\n",
    "# Display results\n",
    "for i, indices in enumerate(topk_indices):\n",
    "    print(f\"\\nQuery image: {query_paths[i]}\")\n",
    "    print(\"Top {} retrieved gallery images:\".format(top_k))\n",
    "    idx_last_slash = str(query_paths[i]).rfind(\"/\")\n",
    "    submission[str(query_paths[i][idx_last_slash+1:])] = list()\n",
    "    for rank, gallery_idx in enumerate(indices):\n",
    "        print(f\"  Rank {rank+1}: {gallery_paths[gallery_idx]}\")\n",
    "        idx_last_slash_res = str(gallery_paths[gallery_idx]).rfind(\"/\")\n",
    "        submission[str(query_paths[i][idx_last_slash+1:])].append(gallery_paths[gallery_idx][idx_last_slash_res+1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe474442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4597118805213184.jpg': ['painting_085_000045.jpg',\n",
       "  'painting_085_000118.jpg',\n",
       "  'painting_085_000084.jpg',\n",
       "  'n01855672_4393.jpg',\n",
       "  'n01855672_1037.jpg'],\n",
       " 'n01855672_10973.jpg': ['n01855672_1037.jpg',\n",
       "  'n01855672_4197.jpg',\n",
       "  'n01855672_4393.jpg',\n",
       "  'painting_085_000045.jpg',\n",
       "  'painting_085_000084.jpg']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcac12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.submission import create_dict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "273fc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query image: ../../data/test/query/4597118805213184.jpg\n",
      "Top 5 retrieved gallery images:\n",
      "  Rank 1: ../../data/test/gallery/painting_085_000045.jpg\n",
      "  Rank 2: ../../data/test/gallery/painting_085_000118.jpg\n",
      "  Rank 3: ../../data/test/gallery/painting_085_000084.jpg\n",
      "  Rank 4: ../../data/test/gallery/n01855672_4393.jpg\n",
      "  Rank 5: ../../data/test/gallery/n01855672_1037.jpg\n",
      "\n",
      "Query image: ../../data/test/query/n01855672_10973.jpg\n",
      "Top 5 retrieved gallery images:\n",
      "  Rank 1: ../../data/test/gallery/n01855672_1037.jpg\n",
      "  Rank 2: ../../data/test/gallery/n01855672_4197.jpg\n",
      "  Rank 3: ../../data/test/gallery/n01855672_4393.jpg\n",
      "  Rank 4: ../../data/test/gallery/painting_085_000045.jpg\n",
      "  Rank 5: ../../data/test/gallery/painting_085_000084.jpg\n"
     ]
    }
   ],
   "source": [
    "dictionary = create_dict_submission(gallery_embeddings, query_embeddings, similarity_matrix, query_paths, gallery_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59964cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4597118805213184.jpg': ['painting_085_000045.jpg',\n",
       "  'painting_085_000118.jpg',\n",
       "  'painting_085_000084.jpg',\n",
       "  'n01855672_4393.jpg',\n",
       "  'n01855672_1037.jpg'],\n",
       " 'n01855672_10973.jpg': ['n01855672_1037.jpg',\n",
       "  'n01855672_4197.jpg',\n",
       "  'n01855672_4393.jpg',\n",
       "  'painting_085_000045.jpg',\n",
       "  'painting_085_000084.jpg']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
